---
phase: 05-parallel-processing
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - ballot_ocr.py
  - batch_processor.py
autonomous: true
must_haves:
  truths:
    - "User can submit 100-500 ballot images in a single batch and receive complete results"
    - "System processes multiple ballots concurrently using ThreadPoolExecutor"
    - "System stays under 2 requests/second API rate limit"
    - "System automatically retries failed API requests with exponential backoff"
    - "System maintains 100% OCR accuracy when processing in parallel"
  artifacts:
    - path: "batch_processor.py"
      provides: "BatchProcessor class with ThreadPoolExecutor and rate limiting"
      exports: ["BatchProcessor", "RateLimiter"]
      min_lines: 100
    - path: "ballot_ocr.py"
      provides: "Refactored extraction function for parallel use"
      contains: "extract_ballot_data_with_ai"
  key_links:
    - from: "batch_processor.py"
      to: "ballot_ocr.py"
      via: "import extract_ballot_data_with_ai"
      pattern: "from ballot_ocr import extract_ballot_data_with_ai"
    - from: "BatchProcessor.process_batch"
      to: "RateLimiter"
      via: "rate_limiter.acquire()"
      pattern: "rate_limiter\\.acquire"
---

<objective>
Implement BatchProcessor class with ThreadPoolExecutor for concurrent ballot processing, rate limiting to stay under OpenRouter API limits (2 req/sec), and automatic retry with exponential backoff for failed requests.

Purpose: Enable processing of 100-500 ballots in minutes instead of hours while respecting API rate limits and maintaining OCR accuracy.
Output: New batch_processor.py module with BatchProcessor class, integrated with existing ballot_ocr.py
</objective>

<execution_context>
@/Users/nat/.claude/get-shit-done/workflows/execute-plan.md
@/Users/nat/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/STACK.md
@.planning/research/ARCHITECTURE.md

# Existing codebase
@ballot_ocr.py (lines 654-739 for extract_ballot_data_with_ai function)

# Key context from research:
# - OpenRouter free tier: 50 req/day, 20 RPM
# - Use ThreadPoolExecutor (I/O-bound API calls benefit from threads under GIL)
# - Rate limiting with semaphore and time-based throttling (2 req/sec)
# - tenacity library for exponential backoff retry
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create RateLimiter class for API rate control</name>
  <files>batch_processor.py</files>
  <action>
Create a new file `batch_processor.py` with a `RateLimiter` class that enforces API rate limits.

Implementation:
1. Create `RateLimiter` class using `threading.Lock` and timestamp tracking
2. Constructor accepts `requests_per_second` parameter (default 2.0 for OpenRouter)
3. Implement `acquire()` method that:
   - Uses a lock to ensure thread safety
   - Tracks last request timestamp
   - Blocks if minimum interval between requests has not elapsed
   - Uses `time.sleep()` for waiting
4. Add `__enter__` and `__exit__` for context manager support

Do NOT use asyncio - this module uses ThreadPoolExecutor with threads, not async/await.
The rate limiter must be thread-safe because multiple threads will call it concurrently.
  </action>
  <verify>
    ```bash
    python -c "
from batch_processor import RateLimiter
import time

# Test rate limiter
limiter = RateLimiter(requests_per_second=2.0)
start = time.time()
for i in range(3):
    with limiter:
        pass
elapsed = time.time() - start
print(f'3 requests took {elapsed:.2f}s (expected ~1.0s)')
assert 0.9 < elapsed < 1.5, f'Rate limiting not working: {elapsed}'
print('RateLimiter test passed')
"
    ```
  </verify>
  <done>RateLimiter class enforces configurable requests-per-second limit with thread-safe implementation</done>
</task>

<task type="auto">
  <name>Task 2: Create BatchProcessor with ThreadPoolExecutor and retry logic</name>
  <files>batch_processor.py</files>
  <action>
Add `BatchProcessor` class to `batch_processor.py` with ThreadPoolExecutor for concurrent processing.

Implementation:
1. Import `concurrent.futures.ThreadPoolExecutor`, `tenacity` library
2. Create `BatchProcessor` class with:
   - Constructor accepting `max_workers` (default 5), `rate_limit` (default 2.0)
   - Initialize internal `RateLimiter` instance
3. Implement `process_single(self, image_path: str) -> Optional[BallotData]`:
   - Uses `@retry` decorator from tenacity with:
     - `stop=stop_after_attempt(3)` (max 3 retries)
     - `wait=wait_exponential(multiplier=1, min=4, max=10)` (4-10 second exponential backoff)
     - `retry=retry_if_exception_type((requests.HTTPError, ConnectionError))`
   - Calls `rate_limiter.acquire()` before API call
   - Imports and calls `extract_ballot_data_with_ai(image_path)` from ballot_ocr
   - Returns BallotData or None
4. Implement `process_batch(self, image_paths: list[str]) -> tuple[list, list]`:
   - Creates ThreadPoolExecutor with max_workers
   - Submits all `process_single` tasks
   - Collects results as they complete using `as_completed()`
   - Returns tuple of (results, errors) where errors is list of {path, error}
5. Add dataclass `BatchResult` with fields: `results: list[BallotData]`, `errors: list[dict]`, `total: int`, `processed: int`

The ThreadPoolExecutor is appropriate here because:
- API calls are I/O-bound (waiting for network response)
- Image encoding is fast (not CPU-intensive)
- Threads work well under GIL for I/O operations
</action>
  <verify>
    ```bash
    python -c "
from batch_processor import BatchProcessor, RateLimiter
import inspect

# Verify BatchProcessor exists and has required methods
bp = BatchProcessor(max_workers=3)
assert hasattr(bp, 'process_single'), 'Missing process_single method'
assert hasattr(bp, 'process_batch'), 'Missing process_batch method'
assert hasattr(bp, 'rate_limiter'), 'Missing rate_limiter'
assert isinstance(bp.rate_limiter, RateLimiter), 'rate_limiter wrong type'

# Verify signature
sig = inspect.signature(bp.process_batch)
params = list(sig.parameters.keys())
assert 'image_paths' in params, 'Missing image_paths parameter'

print('BatchProcessor structure verified')
"
    ```
  </verify>
  <done>BatchProcessor class with ThreadPoolExecutor, rate limiting, and exponential backoff retry is implemented and testable</done>
</task>

<task type="auto">
  <name>Task 3: Add CLI integration for batch processor</name>
  <files>ballot_ocr.py, batch_processor.py</files>
  <action>
Integrate BatchProcessor with the existing CLI in ballot_ocr.py.

Changes to ballot_ocr.py (main function around line 4025):
1. Add argparse argument: `--parallel` flag to enable parallel processing
2. Add argparse argument: `--workers` with default 5 for number of concurrent workers
3. In the batch processing section (around line 4044-4071):
   - When `--parallel` flag is set, import BatchProcessor
   - Create BatchProcessor with specified workers
   - Call `process_batch(image_paths)` instead of sequential loop
   - Handle the (results, errors) return tuple
   - Print summary of errors if any

Changes to batch_processor.py:
1. Add `process_batch_sequential(image_paths: list[str]) -> tuple[list, list]`:
   - Sequential fallback for comparison testing
   - Uses same rate limiting and retry logic
   - Processes one image at a time

Keep backward compatibility: default behavior (no --parallel) should remain sequential.
</action>
  <verify>
    ```bash
    python ballot_ocr.py --help | grep -E "(--parallel|--workers)" && echo "CLI arguments added"

    # Verify import works
    python -c "
from batch_processor import BatchProcessor
bp = BatchProcessor()
print('BatchProcessor import works')
"
    ```
  </verify>
  <done>CLI supports --parallel flag for concurrent processing with configurable --workers count</done>
</task>

</tasks>

<verification>
1. RateLimiter enforces 2 requests/second limit (3 requests take ~1 second)
2. BatchProcessor can be instantiated with configurable workers
3. CLI accepts --parallel and --workers arguments
4. Existing tests still pass (no regression)
</verification>

<success_criteria>
- RateLimiter class with thread-safe rate limiting
- BatchProcessor class with ThreadPoolExecutor (max_workers configurable)
- Retry logic with exponential backoff using tenacity
- CLI integration with --parallel and --workers flags
- No changes to OCR accuracy (same extract_ballot_data_with_ai function)
</success_criteria>

<output>
After completion, create `.planning/phases/05-parallel-processing/05-01-SUMMARY.md`
</output>
